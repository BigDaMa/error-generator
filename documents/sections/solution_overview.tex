\section{Solution Overview}
\label{sec:solution_overview}

Figure~\ref{figure:workflow} illustrates the workflow of our error detection system ED2. 
The system takes a dirty dataset $D$ as input~\ding{182}. 
Based on this dirty dataset, the \emph{Feature Extractor} generates content and metadata features $\rho$ for each data cell. 
The \emph{Feature Extractor} combines information of all cells of the corresponding tuple and the corresponding column to one feature vector for each data cell~\ding{183}.
The feature representation is discussed in detail in Section~\ref{sec:features}.

With the help of the \emph{Initializer}~\ding{184}, the user provides an initial set of labeled cells for both classes, erroneous and correct, for all columns. We train one classifier for each column. This method is described in Section~\ref{sec:init}.
Based on the labeled data provided by the user~\ding{185}, ED2 uses cross-validation to find the optimal set of hyperparameters~\ding{186}. 
ED2 uses these hyperparameters to train an error detection classifier on all available labeled data cells for each column. 
Afterward, ED2 applies this model to all data cells of the corresponding column and estimates the probability of a cell to be erroneous~\ding{187}. 
ED2 leverages these predictions $P$ to augment the feature matrix~\ding{188}. 
This feature augmentation allows the models of different columns to share their knowledge with each other, as we explain in Section~\ref{sec:error_correlation_features}. 

When all models are initialized~\ding{187}, the actual active learning process starts.
Our two-dimensional active learning policy is implemented via the \emph{Column Selector} component and the \emph{Batch Generator} component. 
As we train one classifier per column, the \emph{Column Selector} has to choose the column that should be labeled next~\ding{189}. In Section~\ref{sec:order}, we describe how the \emph{Column Selector} leverages the results of the models to make this decision.
Then, the \emph{Batch Generator} selects the most promising cells for the given column~\ding{190} as described in Section~\ref{sec:uncertaintysampling}. 
For each data cell in the batch, its corresponding tuple is presented one by one to the user~\ding{185}. 
Based on the complete tuple, the user decides whether or not the marked cell value is erroneous. 
After the batch of cells is labeled, the new labels are added to the training set of the corresponding column, hyperparameters are optimized~\ding{186}, and the classifier is retrained on the new data~\ding{187}.
From this point on, the process continues and repeats the steps from~\ding{185} to~\ding{190} in a loop.

During the entire active learning process, the \emph{Status Report} provides the user with a summary of the current state of the dataset. The \emph{Status Report} contains information that correlate with the convergence of the models, such as the certainty distribution for each column and current predictions of the least certain cells as described in Section~\ref{sec:stopAL}.
The active learning loop continues until the user is satisfied with the \emph{Status Report}. Then, the system applies the latest classification models for each column, marks the errors, and returns the result to the user. 

In the following Section, we explain the classification algorithm in more detail, describing the features that we use to model the data and the active learning strategy that we employ to converge to a satisfactory state quickly.
