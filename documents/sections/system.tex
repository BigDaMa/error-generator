\section{Error Classification}
\label{sec:system_all}
We describe how ED2 solves the two main challenges of addressing error detection as a classification task. First, we describe our feature representation. Then, we present our approach for two-dimensional multi-classifier active learning that judiciously chooses the cells that require the user feedback the most. 

\subsection{Feature Representation}
\label{sec:features}
Relational data can consist of numbers, dates, structured text, unstructured text, and categorical data. Our feature representation $\rho$ has to fit all these data types. In the following, we first present two text feature representations. Then, we propose metadata features that provide additional information to extend the language model representation. Furthermore, we show how to combine features of different columns to leverage inter-column correlations. Additionally, we present a feature representation that enables ED2 to exploit error correlation patterns. Finally, we show how the \emph{Feature Extractor} combines the described features into one feature vector per data cell.

\subsubsection{Language Model Features}
A common way to represent text is the bag-of-words representation~\cite{harris1954distributional}. However, most of the relational data does not consist of words, but numbers, dates, and many more diverse types of data. Furthermore, some data errors, such as common keyboard misclicks, can often be discovered only at a finer granularity.
Therefore, instead of a word-level language model, we decide to use a character-level language model $\rho_{\text{lang}}$ for each column in the dataset.
Here, we investigate two competing character-level language models from information retrieval: bag-of-n-grams~$\rho_{\text{n-grams}}$ and long-short term memory (LSTM)~$\rho_{\text{LSTM}}$~\cite{cavnar1994n,hochreiter1997long}. 

\begin{description}
\item[Bag-of-n-grams.]
\label{subsec:bagofcharacters}

The bag-of-n-grams model is a character-based n-gram language model~\cite{cavnar1994n}. Here, the \emph{Feature Extractor} counts all character sequences of length~$n$ occurring in a given column of the dataset. In the case of a unigram model ($n=1$), the \emph{Feature Extractor} counts the occurrences of single characters, such as 'A', 'a', '1', '.', and ' ', for each cell $D[i,j]$ of a given column $D[:,j]$. The \emph{Feature Extractor} applies the common approach of the TF-IDF score to normalize the n-gram counts~\cite{sparck1972statistical}. Formally,

\begin{equation} \label{equation:feature_ngrams}
\resizebox{\hsize}{!}{
$\rho_{\text{n-grams}}(D[i,j],n) = \{ tf(\omega, D[i,j]) \times idf(\omega, D[:,j]) | \omega \in \Omega_{n}(D[:,j]) \},$
}
\end{equation}
where $\Omega_{n}(D[:,j])$ is the set of all n-grams~$\omega$ of length~$n$ that exist in column~$D[:,j]$. 
The term frequency~$tf$ measures the number of times that a specific n-gram~$\omega$ occurs in the cell~$D[i,j]$ and the inverse document frequency~$idf$ measures whether a specific n-gram~$\omega$ is common or rare across all cells in the column~$D[:,j]$. 
Therefore, the feature representation~$\rho_{\text{n-grams}}$ for each data cell~$D[i,j]$ is the list of the TF-IDF scores of all n-grams~$\omega$ in the corresponding column~$D[:,j]$.

Choosing $n$ is not trivial. The unigram text representation~($n=1$) does not provide any order information, such as which character frequently succeeds or precedes certain other characters. Increasing the n-gram length to $n>1$ increases the amount of order information within the n-gram model. However, increasing the n-gram length also results in a significant expansion of the number of features that causes longer model training and overall runtime. 
Therefore, we try to find a trade-off that works well for most of the datasets. 
An alternative to the n-gram model is the LSTM, which retains order information without the significant expansion of the feature space.

\item[Long-Short Term Memory.]

State-of-the-art text classifiers use deep neural networks to train character-level language models~\cite{radford2017learning, zhang2015character}. One network architecture to model language is a recurrent neural network, such as LSTM~\cite{hochreiter1997long}. The LSTM processes text as a sequence of characters. For each character, the model updates its hidden state and predicts the next possible character. 

To create a language model for each column of the dataset, we consider the content of each column as a separate text corpus. We split the text of each column into a training, a validation, and a test set (8 : 1 : 1). Then, we leverage grid-search to find the best hyperparameter configuration of the LSTM model. The approach results in one character-level LSTM language model~$\delta_{j}(D[:,j])$ for each column~$j$. 
Next, we feed forward the character sequence of each cell $D[i,j]$ through the corresponding trained network~$\delta_{j}(D[:,j])$ of column~$j$. Then, we use the final states of the hidden LSTM layer as the feature representation~$\rho_{\text{LSTM}}(D[i,j])$~\cite{radford2017learning}. 
Similar to n-grams of higher degree, an LSTM can model order but requires a much smaller feature space. 
\end{description}

The experiments in Section~\ref{sec:feature_selection_experiment} show that both character-level language models exhibit similar performance. However, the LSTM features require significantly more time than bag-of-n-grams features because of the required training phase. Furthermore, experiments show that there is no significant benefit of using n-grams with $n>1$. Therefore, we set $\rho_{\text{lang}}(D[i,j]) = \rho_{\text{n-grams}}(D[i,j],n=1)$.

\subsubsection{Metadata Features}
\label{sec:explain_metadata_features}
Inspired by Pit-Claudel et al.~\cite{pit2016outlier}, we leverage tuple-expansion rules, such as returning the string length of a cell value or whether a value is numerical, as features. We deploy these metadata features because they either advance the language model or help the user to understand intuitively how the classification model reasons.
However, we use more general features than the ones presented by Pit-Claudel et al.~\cite{pit2016outlier}. For instance, they check whether a value is a valid email address. We want to use features that apply to most datasets and can cover a large number of error patterns. 
Thus, for each cell, we add the following metadata features: 

\begin{description}
  \item[Value Occurrence Count $\rho_{\text{occurrence}}$.] For each distinct value in a column~$D[:,j]$, we count its occurrences. This feature also acts as outlier detection signal.
  \item[String Length $\rho_{\text{string\_length}}$.] The string length of the cell value~$D[i,j]$ can also help to uncover outliers. Even for numbers, the string length might help to find the ones that are far apart from the distribution.
  Using the TF-IDF transformation, we lose the information about the exact string length of each value. Thus, the string length feature extends the language model.
  \item[Is Numerical $\rho_{\text{numerical}}$.] This is a binary feature that tells us whether the cell value~$D[i,j]$ can be represented as a floating point number. This feature helps for example in the case of typographical errors or optical character recognition (OCR) mistakes.
  \item[Is Alphabetical $\rho_{\text{alphabetical}}$.] This is a binary feature that validates whether the value of the cell $D[i,j]$ is alphabetical. This feature also helps in the case of typographical errors or OCR mistakes.
  \item [Floating-point representation $\rho_{\text{number}}$.] This is a sparse feature that contains the floating-point representation of the cell value~$D[i,j]$ whenever it is possible to parse a number. This feature enables the classifier to better understand the underlying error distribution for numerical values. 
  The language model is not able to interpret the value as a number. For instance, the value~\emph{"4321"} is represented in the unigram model as a set of characters~\emph{\{'1','2','3','4'\}}. So, a model is not able to decide whether this value is smaller or greater than another number. By interpreting each suitable value as a number, we enable the classification model, such as an XGBoost~\cite{chen2016xgboost}, to perform numerical comparisons.
\end{description}

These features are represented in a metadata feature vector as follows:

\begin{scriptsize}
\begin{equation} \label{equation:feature_meta}
\begin{split}
\rho_{\text{meta}}(D[i,j]) = \Big[\rho_{\text{occurrence}}(D[i,j]), \rho_{\text{string\_length}}(D[i,j]), \\ \rho_{\text{numerical}}(D[i,j]), \rho_{\text{alphabetical}}(D[i,j]), \rho_{\text{number}}(D[i,j])\Big].
\end{split}
\end{equation}
\end{scriptsize}


Of course, this small set of features is not exhausting the space of all possible features, but it is general enough to capture fundamental characteristics of most datasets. Theoretically, we could let the user add more dataset-specific features. 

\subsubsection{Column-wise Feature Concatenation}
So far, we described features that incorporate information about the current cell and its corresponding column by using a language model and metadata. However, in some cases, it is also necessary to incorporate information about other cells in the same row.
Table~\ref{tab:columnconcat} shows an example to illustrate that inter-column correlations can be essential to detect errors. 

\begin{table}[h!]
	\centering
	\caption{Example for an error that can be detected only by exploiting inter-column correlations.}
	\label{tab:columnconcat}
	\begin{tabular}{|l|l|r|} \hline
		Name & Position & Salary \\ \hline
		Bob & senior manager & \cellcolor{red!50} \textbf{5,000} \\ \hline
		Ann & senior manager & 10,000 \\ \hline
		Joe & junior engineer & 4,000 \\ \hline
		Claudia & senior engineer & 11,000 \\ \hline
	\end{tabular}
\end{table}

The red-colored data cell is erroneous. We see that the value~$5,000$ is within the domain value range. Thus, we cannot detect it using outlier detection or pattern detection on the \emph{Salary} column alone. To be able to detect this error, we have to provide the classifier with the information from the \emph{Position} column. Given sufficient examples provided by the user, the classification model can learn the correlation between the text feature representation of the \emph{senior} position and the floating point representation of a salary of more than $10,000$. Since we do not know what correlations exist in the corresponding dataset, we add features from all columns and let the classifier decide which of the correlations are useful.

Therefore, we concatenate the features of all $M$~columns. Formally,

\begin{equation} \label{equation:feature_concat}
\rho_{\text{concat}}(D[i,j], \rho) = \Big[\rho(D[i,1]), \dots, \rho(D[i,M])\Big].
\end{equation}

This concatenation enables the classifier to detect errors that have dependencies across multiple columns. One example of inter-column relationships is functional dependencies. 
So, the machine learning model has not only access to the content of the cell in question, but also to the content of all other cells of the corresponding row. Thus, the model can exploit inter-column correlations. 

\subsubsection{Error Correlation Features}
\label{sec:error_correlation_features}
%Daniel Defoe:
Misfortunes seldom come alone --- the same often applies to errors. Once, a classifier detected an error in one of the attributes of a tuple, it is more likely that there is an error in another attribute of the same tuple as well.
In particular, that happens when certain tuples come from an unreliable source.
So far, each classification model can access only the labels of its corresponding column $j$ and therefore does not know about errors in the other columns $k$.
 We want to incorporate knowledge about errors in other columns. Therefore, for our current column $j$, we concatenate the predicted probabilities~$P_{\phi_{k}}(D[i, k] = erroneous)$ for all dataset cells from previous learning iterations, where $j\neq k$. Formally,

\begin{scriptsize}
\begin{equation} \label{equation:feature_errorcorrelation}
\begin{split}
\rho_{\text{corr}}(D[i,j]) = \Big[P_{\phi_{1}}(D[i, 1] = \text{erroneous}), \dots , P_{\phi_{j-1}}(D[i, j-1] = \text{erroneous}), \\ 
P_{\phi_{j+1}}(D[i, j+1] = \text{erroneous}) , \dots , P_{\phi_{M}}(D[i, M] = \text{erroneous})\Big].
\end{split}
\end{equation}
\end{scriptsize}

Our experiments show that the classification model can leverage this knowledge to narrow down the search space for errors. Consequently, it can sample more errors in the first labeling iterations and converge more quickly. We add these features to the feature matrix in step~\ding{188} in Figure~\ref{figure:workflow}.

\subsubsection{Feature Vector at a Glance}

For the \emph{Feature Extractor}, we choose the feature combination of unigrams, metadata, and error correlation features. Furthermore, we apply the column-wise concatenation of features. Formally, we can define the final feature vector per data cell~$D[i,j]$ as follows:

\begin{equation} \label{equation:feature_all}
\resizebox{.9\hsize}{!}{
$\rho_{\text{ED2}}(D[i,j]) = \bigg[\rho_{\text{concat}}(D[i,j], \rho_{\text{lang}}), \rho_{\text{concat}}(D[i,j], \rho_{\text{meta}}), \rho_{\text{corr}}(D[i,j])\bigg].$
}
\end{equation}

This feature combination shows the best trade-off among convergence, $F_{1}$-score, feature generation time, training time, and interpretability as shown in the experiments in Section~\ref{sec:feature_selection_experiment}.



\subsection{Two-Dimensional Active Learning}
\label{sec:AL}
The user has to label a set of cell values of each column to train one classifier for each column. The active learning approach tailored to our use case follows three steps. First, we present the \emph{Initializer} that helps to overcome the cold start problem. Second, we discuss the \emph{Batch Generator} that selects cells within a column. Third, we explain the \emph{Column Selector} that directs the user on which columns to focus her labeling effort.

\subsubsection{Initialization}
\label{sec:init}
In order to be able to train a classifier, we need positive and negative examples for erroneous cells. As a minimum, we should start active learning with two erroneous cells and two correct cells per column. However, in the beginning, there are no labeled cells.

The naive approach is to let the user browse through the table and classify these cells to start up the algorithm. 
A more advanced approach is to run a black-box outlier detection strategy and let the user label its output. As it is our design decision to have a parameter-free method for the ease of the user, we decided to rank the values based on the frequency, which is the task of the \emph{Initializer}. We assume that correct cells are more likely to occur frequently and erroneous cells rather rarely. 

\subsubsection{Distinct Batch Uncertainty Sampling}
\label{sec:uncertaintysampling}
In an active learning setting, we try to mimic how the user labels cells. 
We use an error classification model that predicts the class (erroneous or correct) for each cell. Additionally, the classifier has to provide the probability or confidence score for the corresponding prediction. This probability corresponds to the \emph{certainty} of our model. 
To select new cells for labeling, the \emph{Batch Generator} chooses the $k$ least certain cells. 
We decided to choose cells with distinct values because cells with the same content tend to have similar certainty. If we would not require distinctness for the batch, the user will often label the same value in many different tuples. If there are not enough distinct values to fill the batch, the \emph{Batch Generator} fills the batch with any least certain value.

To configure the parameters of the classification model, we apply hyperparameter optimization using cross-validation on the corresponding labeled training set (step~\ding{186} in Figure~\ref{figure:workflow}). To keep runtime low, we apply hyperparameter optimization only in the first round.

Each time that ED2 receives new feedback from the user for a certain column, we train a model on all labeled examples that are available for this column (step~\ding{187} in Figure~\ref{figure:workflow}). Then, we use the newly trained classifier to predict for all cells of the corresponding column whether or not they contain an error. The active learning loop continues until the user is satisfied by the state of the error detection model.


\subsubsection{Column Selection}
\label{sec:order}

We train one error classification model for each column. Therefore, in addition to sampling cells within a column, we need a strategy for the \emph{Column Selector} to decide the next column for user labeling.

A naive approach for column selection is the random column selection strategy~(RA). This strategy randomly chooses one column and lets the user label one batch of records for this selected column. A more orderly approach is the selection in a round-robin fashion~(RR). So, we assign the batches to the user for all columns in equal portions and in a circular order.


Both of the presented naive strategies do not consider that for some columns, the model might detect errors easily and converge quickly, whereas for other columns there might be more complex, diverse errors that cause the model to converge slower. Slower convergence also means that the user needs to label more records to achieve the same performance. Figure~\ref{fig:column_progress} illustrates one example of significant differences between the convergence behavior of two columns. In this example, an optimal column selection strategy would choose to label the column \emph{City} only for two iterations because after that the classifier is already relatively accurate in classifying the column values. Then, the strategy would continue to ask the user for labels for the column \emph{MiddleName}.

\begin{figure}[H]
\centering
\input{charts/columns_all/columns}
\caption{Example for differences in model convergence between two columns.} 
\label{fig:column_progress}
\end{figure}

We studied three strategies to choose the next best column. These column selection strategies take the convergence of the models into account to optimize the global $F_1$-score for the corresponding dataset.
There are several column selection strategies possible:

\begin{description}
  \item[Min Certainty (MC).] The classification model returns a probability score for each prediction, i.e., the certainty. High certainty correlates with model convergence~\cite{zhu2007active}. We calculate the average certainty for all cells in a column and choose the column with the lowest average certainty. 
  \item[Max Error (ME).] In each iteration, we apply cross-validation on the current labeled training set for the corresponding column. The cross-validation scores are an estimate of the overall performance on the whole column data and thereby also correlate with convergence~\cite{zhu2007active}. We calculate the average of all cross-validation scores and choose the column that performed worst with respect to this average.
  \item[Max Prediction Change (MP).] Prediction change is the fraction of predictions that changed compared to the previous iteration. Prediction change correlates with active learning convergence~\cite{bloodgood2009method}. We choose the column with the greatest prediction change.
\end{description}

To gather all these metrics, we apply one round of round-robin first. Then, we can use these metrics to apply one of the three strategies for column selection.

For the \emph{Column Selector}, we choose Min Certainty because it is most stable against local optima. For instance, if we use the Max Error strategy, we might achieve 100\% $F_{1}$-score on the labeled training set, but still perform poorly on the overall dataset. Also, in the case of the Max Prediction Change strategy, minor or no prediction change does not necessarily mean no learning progress. 
The experiments in Section~\ref{subsec:orderstrategy} show that the Min Certainty strategy is effective across all datasets.

\subsubsection{Stopping Criteria}
\label{sec:stopAL}

The users have to decide on their own when to stop the active learning procedure. To support the users in their decision and to provide them with a good understanding of the current state of the dataset, we present the following statistics in the \emph{Status Report} at every stage of the procedure:

\begin{itemize}
  \item The $F_{1}$-score, precision, and recall on all labeled cells per column.
  \item The predictions for the top $k$ least certain and most certain dataset cells per column. Additionally, the user can also browse the entire error detection result.
  \item A ranking of the most decisive features with regard to information gain for each classification model to understand better whether the underlying model works correctly.
  \item The certainty distribution of each model for each column.
  \item The error detection cross-validation score on the labeled data for each column.
  \item The prediction change of all previous iterations for each column.
\end{itemize}

Based on all this information, the user has an overview of the current state of the dataset and can assess when to stop.





