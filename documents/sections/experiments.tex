\section{Experiments}
\label{sec:evaluation}

We conducted several experiments to compare our approach against state-of-the-art error detection methods on multiple datasets and to benchmark different aspects of our solution.
First, we describe the experimental setup. Then, we investigate how our approach compares to state-of-the-art error detection methods in terms of effectivity, user effort, and runtime. Finally, we explore the influence of different feature representations and models on the performance of ED2.


\subsection{Experimental Setup}
\label{sec:experimental_setup}

In the following, we present the datasets, competing methods, evaluation methodology, parameter configuration, and implementation details.

\subsubsection{Datasets}

We conducted our experiments on several datasets that have been used in prior work in data cleaning. 
Table~\ref{tab:datasets} lists these datasets along with the number of rows, number of columns, and the corresponding fraction of erroneous cells divided by all cells in the dataset. As shown, the datasets vary in both shape and number of errors.

\begin{description}
  \item[Address.] Address is an anonymized proprietary dataset with common attributes, such as name, address, city, postal code~(ZIP), and social security number. Errors are introduced synthetically by randomly inserting errors that concern spelling, formatting, completeness, and field separation. \textit{We included this dataset to study error detection performance on a wide variety of formats and data types.}
  \item[Flights.] Li et al. crawled information on the departure and arrival time of flights from 38 different websites~\cite{li2012truth}. In addition to the flight information, the data also contains the corresponding source for each record. Errors comprise mostly wrong departure/arrival times and missing values.
  Rekatsinas et al. provided us with both, the dataset and the corresponding ground truth~\cite{rekatsinas2017holoclean}. \textit{We use this dataset to examine how stable our algorithm is in the presence of large numbers of errors, and to evaluate if our system can successfully exploit inter-column correlations.}
  \item[Hospital.] Hospital is one of the most commonly used datasets to benchmark data cleaning algorithms~\cite{rekatsinas2017holoclean, chu2013holistic, dallachiesa2013nadeef}. The data originates from the US Department of Health \& Human Services. Errors are introduced synthetically by randomly replacing characters by the letter 'x'. Rekatsinas et al. provided us with both, the dataset and the corresponding ground truth~\cite{rekatsinas2017holoclean}. \textit{We use this dataset to show that our method can achieve state-of-the-art accuracy on a well-established error detection benchmark.}
\end{description}

\begin{table}[t]
\centering
\caption{Experimental data sets.}
\label{tab:datasets}
\begin{tabular}{|l|r|r|r|r|} \hline
Dataset& Total Columns& Dirty Columns& Rows& Errors\\ \hline
Address& 12& 7 & 94306& 13.87\%\\ \hline
Flights& 6& 4 & 2376& 34.51\%\\ \hline
Hospital& 17& 17 & 1000& 2.99\%\\ \hline
\end{tabular}
\end{table}



\subsubsection{Baselines}
We evaluate ED2 against four competing error detection approaches:

\begin{description}
  \item[NADEEF. \label{nadeef_config}] NADEEF~\cite{dallachiesa2013nadeef, khayyat2015bigdansing} is a rule violation detection system  that allows the users to specify multiple types of declarative rules, such as functional dependencies and denial constraints, to detect data errors. \\
  \textbf{Usage:}
  We run NADEEF leveraging functional dependencies. For the Hospital dataset, we use the functional dependencies provided by Dallachiesa et al.~\cite{dallachiesa2013nadeef}. For the Address dataset, we apply the functional dependencies provided by the owner of the dataset. For the Flights dataset, we apply the data profiling tool Metanome~\cite{papenbrock2015metanome} on the clean data to find all functional dependencies. We consider a cell erroneous if it participates in at least one violation~\cite{abedjan2016detecting}.
  
  Note that to use rule-based methods, the user has to be aware of existing correct functional dependencies. For ED2, the user neither has to understand the concept of functional dependencies nor how to detect and formulate them.
  
  
  \item[dBoost.] dBoost~\cite{pit2016outlier} is a framework that provides various machine learning models to detect outliers. 
  dBoost proposes tuple expansion to leverage rich information from semantically poor SQL data types such as strings, integers, and floating point numbers. \\
  \textbf{Usage:}
  We apply three machine learning models provided by dBoost: Histograms, Gaussian, and Mixtures. These models require hyperparameter optimization, which requires labeled data. Based on the ground truth, we randomly provide the algorithm with a small number of labeled rows. On this labeled dataset, we apply parameter grid search. The resulting best set of parameters is applied to the whole dataset. Since this approach is not deterministic, we apply this procedure 10 times and report the average performance.
  
  Note that to use outlier detection methods, the user has to implement hyperparameter optimization by herself. For ED2, hyperparameter optimization is running without the user's involvement, as we explain in Section~\ref{sec:uncertaintysampling}.
  
  \item[KATARA.] KATARA~\cite{chu2015katara} can be used to detect semantic pattern violations using external knowledge bases.\\
  \textbf{Usage:}
  We provide KATARA with access to the complete knowledge base of DBpedia~\cite{lehmann2015dbpedia}. Thus, potentially it can find violations for the data domains present in the DBpedia knowledge base.
  
  Note that, in contrast to KATARA, our approach does not rely on external resources.
  
  \item[OpenRefine. \label{sec:openrefine}] OpenRefine~\cite{verborgh2013using} is an open source tool for data cleaning and transformation. \\
  \textbf{Usage:} 
  Assuming, that the user already knows all transformations that are necessary to clean the data, 
  we leverage the transformation engine to formulate transformation rules that expose the errors. The rules require knowledge on the corresponding expression language. For example, the rule $if(isNotNull(value.match(/{\backslash}d+{\backslash}:{\backslash}d+ [p,a]{\backslash}.m{\backslash}./)), value, "error")$ marks the cell as erroneous if the cell does not conform  the formulated date format required by the Flights dataset. 
  
  The assumption that the user knows the exact error patterns and how to formulate these into rules, is in general very strong. Basically, this approach assumes that the user already knows the possible errors in the dataset and can formulate corresponding rules. We included this approach as an "optimum user experience" competitor simply to show how close our learning-based approach can get to hand-crafted and data-fitted patterns.
\end{description}

\subsubsection{Evaluation Methodology}
We measure the effectivity of the methods in detecting potential errors using precision~(P), recall~(R), and $F_1$-score~($F_1$) as defined in Section~\ref{sec:problem}.
Additionally, we measure the runtime of each method in minutes. If the algorithm requires labeling, we report the number of labels. 

\subsubsection{Our Approach}
Per default, we use ED2 with the feature combination of unigrams, metadata, and error correlation features, and the \emph{Min Certainty} column selection strategy. Both configurations generally outperform the other options, as we report in our experiments. 
Preliminary experiments show that the batch size does not significantly affect performance. Thus, we choose an active learning batch size of $k = 10$. As the classifier, we choose XGBoost~\cite{chen2016xgboost} because of its robustness against irrelevant features~\cite{friedman2001greedy}, as we discuss in Section~\ref{sec:model_selection}. Whenever we use the LSTM features, we optimize the following hyperparameters: the learning rate, dropout, sequence length, and batch size. Moreover, we always use one LSTM layer with 128~units to reduce the search space and to represent each column by 128~features.
Since ED2 is not deterministic, we apply it 10~times and report the results either as an average or box plot.


\subsubsection{Implementation Details}
Our current prototype is available at \url{https://github.com/BigDaMa/ED2}. All experiments were executed on a machine with 14~2.60GHz Intel Xeon E5-2690 CPUs (each two threads), 251GB~RAM, and running Ubuntu~16.04.2. Furthermore, we train the LSTM networks on a machine with one~GPU (GeForce GTX 980), 16~AMD Opteron 6376 CPUs (each two threads), 62GB~RAM, and running Ubuntu~16.04.3.


\begin{figure*}[ht!]
	\centering
	\input{charts/labels_all/labels}
	\caption{User effort and effectivity analysis of different error detection methods for different datasets.}
	\label{figure:methods_comparison_user_effort}
\end{figure*}


\begin{table}[ht!]
	\centering
	\caption{Best $F_1$-score results of each method in Figure~\ref{figure:methods_comparison_user_effort} and their corresponding precision (P) and recall (R).}
	\label{tab:effectivity}
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{|l?r|r|r?r|r|r?r|r|r|}
		\hline
		
		& \multicolumn{3}{c?}{ Address } & \multicolumn{3}{c?}{ Flights } & \multicolumn{3}{c|}{ Hospital } \\ \hline
		
		& P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$ \\ \specialrule{.1em}{.05em}{.05em}
		
		ED2 & 0.98 & 0.97 & \textbf{0.97} & 0.90 & 0.90 & \textbf{0.90} & 1.00 & 0.93 & 0.96 \\ \hline
		
		Gaussian & 0.23 & 0.64 & 0.34 & 0.37 & 1.00 & 0.54 & 0.82 & 0.68 & 0.75 \\ \hline
		
		Histogram & 0.11 & 0.13 & 0.12 & 0.48 & 0.75 & 0.59 & 0.78 & 0.83 & 0.80 \\ \hline
		
		KATARA & 0.77 & 0.50 & 0.60 & 0.00 & 0.00 & 0.00 & 0.12 & 0.24 & 0.16 \\ \hline
		
		Mixture & 0.37 & 0.33 & 0.34 & 0.52 & 1.00 & 0.68 & 0.03 & 0.04 & 0.03 \\ \hline
		
		NADEEF & 0.37 & 0.38 & 0.38 & 0.49 & 0.75 & 0.59 & 0.03 & 0.36 & 0.06 \\ \hline
		
		OpenRefine & 1.00 & 0.83 & 0.90 & 1.00 & 0.58 & 0.74 & 1.00 & 1.00 & \textbf{1.00} \\ \hline
		
	\end{tabular}
}
\end{table}


\subsection{Effectivity}
\label{sec:effectivity}

In all our experiments, ED2 achieves state-of-the-art error detection $F_1$-score with only a small set of labels.
Figure~\ref{figure:methods_comparison_user_effort} illustrates the $F_1$-score performance of different error detection methods with respect to the number of labels required if applicable. Additionally, Table~\ref{tab:effectivity} reports the best $F_1$-score result of each error detection method from Figure~\ref{figure:methods_comparison_user_effort} to show the corresponding precision and recall. 

For the Address dataset, given the labels of 178~cells (26~cells per erroneous column, 0.02\% of all cells in the dataset), ED2 outperforms all other methods with regard to the $F_1$-score. 
The main reason for this superior performance is that it can detect a broad set of error types, such as pattern violations, outliers, and functional dependency violations that exist in the Address dataset. 

Other methods cannot reach the performance of ED2 because they cannot cover all these error types. OpenRefine as a column-specific pattern checking tool can neither identify outliers, such as minor typos in some of the city names, nor errors that require context from other columns. For instance, due to erroneous field separation, there are erroneous records with content, such as \emph{City} = "LOS", \emph{State} = "ANGELES", and \emph{ZIP} = "CA 90001".
In this case, the classifiers identify that \emph{City} and \emph{State} are erroneous because the \emph{ZIP} is not numerical. This example shows how important context from other columns is.

KATARA can find mismatches only in the domains \emph{ZIP} and \emph{City} because the knowledge base does not cover all the columns of the dataset.
NADEEF achieves a low $F_1$-score due to both, low precision and low recall. The low precision arises from the fact that the approach considers all involved cells of a violation as erroneous and the low recall is due to the fact that the defined functional dependencies do not cover all erroneous columns. 
Outlier detection methods (i.e., Histogram, Gaussian, and Mixture) do not perform well on the Address dataset due to the large fraction of errors in this dataset that modifies the data distribution. For instance, the erroneous city name~\emph{"SAN"} occurs $5051$~times in this dataset.

Also on the Flights dataset, ED2 achieves state-of-the-art error detection $F_1$-score with only 56 labels (14~labels per erroneous column, 0.40\% of all cells in the dataset).
The Flights dataset requires the error detection method to exploit inter-column correlations to achieve high performance. For instance, a particular flight web source might offer more qualitative data for specific flights. The user might not even know about these subtle correlations. For example, while investigating the results, we found that one of the most decisive features for the classification model for the actual arrival time is whether the scheduled departure time contains the character 'p'. It turns out that the probability of the actual arrival time to be erroneous is higher if the corresponding flight departs in the afternoon as denoted by "p.m."~\cite{us2018air}.
Apparently, ED2 learns such correlations effectively.
For the same reason, NADEEF, which also exploits functional dependencies, has a higher performance on this dataset compared to the other datasets.
In contrast to the Address dataset, the dBoost methods show a strong performance because based on the tuple expansion, they consider the large number of missing values as errors.
KATARA performs poorly because the flight domain is not present in the DBpedia knowledge base.

\begin{figure*}[t!]
	\centering
	\subfigure
	{
		\label{figure:runtime_blackoak}
		\input{charts/runtime_bar_charts_server/blackoak}
	}
	\subfigure
	{
		\label{figure:runtime_flights}
		\input{charts/runtime_bar_charts_server/flights}
	}
	\subfigure
	{
		\label{figure:runtime_hospital}
		\input{charts/runtime_bar_charts_server/hospital}
		
	}	
	\caption{Runtime analysis of different error detection methods.}
	\label{figure:methods_comparison_runtime}
\end{figure*}

For the Hospital dataset, given the labels of 568~cells (34~cells per erroneous column, 3.3\% of all cells in the dataset), ED2 outperforms the majority of methods except OpenRefine.
The main reason that ED2 cannot outperform OpenRefine is that all 17~columns of the Hospital dataset contain the same error, namely a randomly inserted 'x'. In other words, by configuring OpenRefine to find 'x' in the dataset, the user can achieve perfect error detection performance, which is a rare and probably unrealistic scenario.
However, we choose to add this dataset because it is a common data cleaning benchmark dataset~\cite{rekatsinas2017holoclean, chu2013holistic, dallachiesa2013nadeef}. 
The reason for the larger number of required labels is that the Hospital dataset has significantly more erroneous columns compared to the other datasets. Since ED2 trains a model for each erroneous column, ED2 also requires labels for each of these columns. 
In general, we see that ED2 converges to the best possible performance and achieves 100\%~precision. 
In summary, we see that, given a small set of labels, ED2 can outperform all other error detection methods if the user does not already know all error patterns upfront. Furthermore, ED2 does not require any tool-specific knowledge.


\subsection{Detection Runtime}
\label{sec:runtime}

Figure~\ref{figure:methods_comparison_runtime} shows the computation time for all tested methods. Notice that the scale of the y-axis differs in each chart.
For ED2, we report the computation time that it takes to outperform all other tools with regard to $F_1$-score.
For all other methods, we choose the configuration that had the highest $F_1$-score reported in Section~\ref{sec:effectivity}.
We do not report the computation time of OpenRefine because the error detection is performed by the user. OpenRefine only applies the user-specified patterns in a filter operation.

ED2 shows comparable runtime as other error detection methods.
For the Address dataset, we see that ED2, the Gaussian model, and the Histogram model all take between three and five minutes of computation time. The mixture model has a higher computation time. High dimensional data affects the mixture model and causes the relative high computation time of 23~minutes. Also, KATARA runs half an hour to process a large dataset, such as the Address dataset.
NADEEF shows the longest computation time. 
For the Flights dataset, due to the small number of rows, the computation time is low for all methods, being less or about one minute. 
Similar to the Flights dataset, the computation time on the Hospital dataset is low for all methods except the Mixture models that suffer from the high dimensionality problem again. 



\subsection{Micro-Benchmark Results}
\label{sec:microbenchmark}

Next, we evaluate different internal decisions that we made for our system.
First, we evaluate different sets of features that we described in Section~\ref{sec:features}. Then, we evaluate different strategies for column selection to optimize the trade-off between user effort and $F_1$-score. Finally, we evaluate different classifiers as the learning model in our system. 

\subsubsection{Feature Representation}
\label{sec:feature_selection_experiment}

Figure~\ref{fig:features} illustrates how the $F_1$-score evolves based on the accumulation of labels for different feature sets.

For the Address dataset, we can see that using the metadata features alone~(M) results in a fast convergence at the beginning. However, we see that, if we enrich the metadata features with language model features, such as unigrams~(UM) or LSTM~(LM) features, we achieve slightly higher $F_1$-score at about 150~labels. If we add the error correlation features~(UME), we see a slight improvement in the beginning of the convergence because these features help the classifier to narrow the search space for errors.
If we use only unigram~(U) features, the $F_1$-score is ${\sim}6\%$ less than the best feature sets. If we add bigrams~(UB), we gain 1\%~$F_1$-score over unigrams. This small gain shows that the character order plays a minor role in detecting errors for the Address dataset.
Furthermore, we see that the word-level unigrams~(WU) perform significantly worse than all other feature sets because this representation loses too much information. 

For the Flights dataset, the feature combination of unigrams, metadata, and error correlation features~(UME) results in the best convergence behavior. Similar to the Address dataset, we see that the combination of language model features and metadata reaches the highest $F_1$-score in the end.
However, using only metadata~(M) features results in an inferior convergence in the beginning. The main reason is that metadata features do not encode from which website a flight originates and thus the classifier can not learn the trustworthiness of each source.

For the Hospital dataset, we see that the feature combination of unigrams, metadata, and error correlation features~(UME) leads to the fastest convergence. As expected, the word-level unigrams perform especially poorly because the character 'x' causes the only error.

In summary, it is more beneficial to use the feature combination of unigrams, metadata, and error correlation features~(UME) due to high $F_1$-score, fast feature generation, fast training, and high interpretability.

\begin{figure*}[h!]
	\centering
	\input{charts/features_all/features}
	\caption{Comparison of different feature sets for all datasets.} 
    \label{fig:features}
\end{figure*}
%\\


\subsubsection{Column Selection Strategy}
\label{subsec:orderstrategy}

We evaluate different sampling strategies for choosing the next column for labeling. The goal of the sampling strategies is to induce the steepest learning curves to achieve a high $F_1$-score as fast as possible. 
Figure~\ref{fig:order} illustrates how the column selection strategies proposed in Section~\ref{sec:order} perform on all datasets. 

The beginning of all graphs, except for the random strategy~(RA), is similar because we always execute one round of round-robin first to gather the necessary statistics for each column, such as certainty, cross-validation scores, and prediction change.


The reason for the superior behavior of the random selection in the beginning is that in the initial step of each column, we are training only on four records. This small training set often results in 0\%~$F_1$-score. Thus, if we choose the round-robin strategy, we see 0\%~$F_1$-score for the first round of active learning. However, the random strategy might select one column multiple times in the beginning and therefore we see a nonzero $F_1$-score earlier. This selection represents only a short-term benefit.

For the Address dataset, we see the most significant effect of column selection strategies compared to the other datasets because the dataset is so diverse in terms of data types and errors. The certainty-based strategy~(MC) leads to the fastest convergence and outperforms other column selection strategies.

For the Flights dataset, due to very similar characteristics across columns, the column order does not make a significant difference. For instance, all date columns exhibit similar formatting issues, and show similarly high error fractions. 
However, we can still see that the strategies based on minimum certainty~(MC) and prediction change~(MPC) outperform the other column selection strategies.

For the Hospital dataset, we see that the minimum certainty~(MC) strategy performs best in the beginning but in the end, the maximum error strategy~(ME) performs slightly better. However, the maximum error strategy~(ME) shows slow convergence in the beginning.

In summary, because of the fast convergence and the high, stable $F_1$-score across datasets, we choose to use the minimum certainty~(MC) sampling strategy for ED2.

\begin{figure*}[h!]
	\centering
	\input{charts/order_all/order}
	\caption{Comparison of different column selection strategies for all datasets.} 
    \label{fig:order}
\end{figure*}





\subsubsection{Model selection}
\label{sec:model_selection}

We evaluate the effect of different classifiers on our system to find the best learning model. These classifiers are the tree boosting algorithm XGBoost~\cite{chen2016xgboost}, multinomial Naive Bayes~\cite{mccallum1998comparison}, and a support vector machine~(SVM) using a linear kernel~\cite{cortes1995support}, which have been used for text classification use cases~\cite{tong2001support, chen2016xgboost, mccallum1998comparison}. Figure~\ref{fig:model} illustrates the result of this evaluation. 

Although, Naive Bayes and SVM perform better on very small training sets, XGBoost achieves the highest $F_1$-score within the fastest convergence on all three datasets. 
The main reason is that trees are robust against irrelevant features and can handle missing values~\cite{friedman2001greedy}. For instance, for the Hospital dataset, only one out of $456$ features is relevant to detect the error, namely whether or not the character 'x' is present. SVMs perform poorly in the case of many irrelevant features~\cite{weston2001feature}. Furthermore, functional dependencies within the data and class imbalance negatively affect the performance of Naive Bayes~\cite{rennie2003tackling}.
In summary, XGBoost appears to be more robust than the competitors for ED2.


\begin{figure}[h]
	\centering
	\input{charts/model_all/models_all}	
	\caption{Comparison of different classification models.} 
    \label{fig:model}
\end{figure}




